{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 09:05:42.082289: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-29 09:05:42.457375: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-29 09:05:43.199378: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.core.common import flatten\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import seaborn as sns\n",
    "import platform\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import glob\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "\n",
    "# import albumentations as A\n",
    "# from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import torch.nn as nn\n",
    "# import torchvision\n",
    "# import torchvision.models as models\n",
    "# import torch.nn.functional as nnf\n",
    "\n",
    "# import timm\n",
    "import cv2\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class CONFIG:\n",
    "    TRAIN_PATH = \"../00_data/dataset/augmented\"\n",
    "    VAL_PATH = \"../00_data/dataset/val\"\n",
    "    MODEL_NAMES = ['densenet161', 'mobilevitv2_150', 'mobilevitv2_150_384_in22ft1k']\n",
    "    BATCH_SIZE = 8\n",
    "    LEARNING_RATE = 1e-4\n",
    "    DROPOUT = 0.3\n",
    "    EPOCHS = 10\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    \n",
    "# def get_device():\n",
    "#     global DEVICE\n",
    "#     if torch.cuda.is_available():\n",
    "#         print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
    "#         DEVICE = torch.device(\"cuda:0\")\n",
    "#     else:\n",
    "#         print(\"\\n[INFO] GPU not found. Using CPU: {}\\n\".format(platform.processor()))\n",
    "#         DEVICE = torch.device(\"cpu\")\n",
    "    \n",
    "RANDOM_SEED = 42\n",
    "set_seed(RANDOM_SEED)\n",
    "# get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "no = np.load(\"../../00_data/original_data/no.npy\")\n",
    "sphere = np.load(\"../../00_data/original_data/sphere.npy\")\n",
    "vortex = np.load(\"../../00_data/original_data/vortex.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_no = np.zeros(len(no))\n",
    "y_sphere = np.ones(len(sphere))\n",
    "y_vortex = np.full(len(vortex), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_no = to_categorical(y_no, num_classes=3)\n",
    "y_sphere = to_categorical(y_sphere, num_classes=3)\n",
    "y_vort = to_categorical(y_vortex, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.concatenate([no, sphere, vortex], axis=0)\n",
    "y_data = np.concatenate([y_no, y_sphere, y_vort], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "del(no,sphere,vortex)\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def convert_grayscale_images_to_rgb(images):\n",
    "    \"\"\"\n",
    "    Convert a batch of grayscale images to RGB by replicating the grayscale\n",
    "    channel three times.\n",
    "\n",
    "    Parameters:\n",
    "    - images: A 3D NumPy array or TensorFlow tensor of shape [samples, height, width].\n",
    "\n",
    "    Returns:\n",
    "    - A 4D TensorFlow tensor of shape [samples, height, width, 3].\n",
    "    \"\"\"\n",
    "    # Add a channel dimension, making it [samples, height, width, 1]\n",
    "\n",
    "    # Replicate the channel 3 times, resulting in [samples, height, width, 3]\n",
    "    images_rgb = tf.tile(images, [1, 1, 1, 3])\n",
    "    return images_rgb\n",
    "\n",
    "# Example usage with your data:\n",
    "x_data = convert_grayscale_images_to_rgb(x_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 150, 150, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Prepare Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2, random_state=RANDOM_SEED, stratify=y_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(x_train, y_train, x_test, y_test, batch_size):\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(len(x_train)).batch(batch_size)\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class MobileVitV2_150(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, n_classes):\n",
    "        super(MobileVitV2_150, self).__init__()\n",
    "        \n",
    "        # Assuming an equivalent model is loaded here.\n",
    "        # TensorFlow Hub could be a source for such models.\n",
    "        self.vit_model = tf.keras.applications.MobileNetV2(input_shape=(150, 150, 1), include_top=False, weights='imagenet')\n",
    "        self.vit_model.trainable = True  # Make the model trainable\n",
    "        \n",
    "        self.classifier = tf.keras.Sequential([\n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            tf.keras.layers.Dense(512, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),  # Assuming using the dropout from CONFIG\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.3),  # Assuming using the dropout from CONFIG\n",
    "            tf.keras.layers.Dense(n_classes, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.vit_model(inputs, training=True)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "class TransferLearningModelNew(tf.keras.Model):\n",
    "    def __init__(self, n_classes):\n",
    "        super(TransferLearningModelNew, self).__init__()\n",
    "        \n",
    "        # Load a pre-trained model. Assuming DenseNet161 equivalent in TensorFlow.\n",
    "        # Note: TensorFlow models by default use 3 input channels. If you need a single-channel model,\n",
    "        # you may need to adjust the input layer or preprocess your input data accordingly.\n",
    "        self.base_model = tf.keras.applications.DenseNet169(include_top=False, input_shape=(150, 150, 1), weights=None)\n",
    "        self.base_model.trainable = True  # Enable training on the base model\n",
    "        \n",
    "        # Assuming the feature extraction from the base model results in a specific shape, adjust accordingly.\n",
    "        # The number of features (2208 * 4 * 4) needs to be updated based on the output shape of your specific base model.\n",
    "        self.global_pool = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.classifier = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(1024, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.33),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.33),\n",
    "            tf.keras.layers.Dense(n_classes, activation='softmax')  # Use 'softmax' for multi-class classification\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.base_model(inputs, training=training)\n",
    "        x = self.global_pool(x)\n",
    "        x = self.classifier(x, training=training)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class DenseNet201(tf.keras.Model):\n",
    "    def __init__(self, n_classes):\n",
    "        super(DenseNet201, self).__init__()\n",
    "        # Initialize the DenseNet201 base model\n",
    "        self.base_model = tf.keras.applications.DenseNet201(include_top=False,\n",
    "                                                            input_shape=(150, 150, 1),\n",
    "                                                            weights='imagenet')\n",
    "        self.base_model.trainable = True  # Set True to fine-tune all layers\n",
    "\n",
    "        # Define the custom classifier\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.classifier = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(1024, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.33),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.33),\n",
    "            tf.keras.layers.Dense(n_classes, activation='softmax')  # Use 'sigmoid' for binary classification\n",
    "        ])\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.base_model(inputs, training=training)\n",
    "        x = self.flatten(x)\n",
    "        x = self.classifier(x, training=training)\n",
    "        return x\n",
    "\n",
    "class DenseNetEnsemble(tf.keras.Model):\n",
    "    def __init__(self, modela, modelb):\n",
    "        super(DenseNetEnsemble, self).__init__()\n",
    "        self.modela = modela\n",
    "        self.modelb = modelb\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        outa = self.modela(inputs, training=training)\n",
    "        outb = self.modelb(inputs, training=training)\n",
    "        # Assuming the models are for classification and use softmax, averaging predictions is a common approach.\n",
    "        # If you're directly adding outputs (e.g., for regression), you might simply use: out = outa + outb\n",
    "        out = (outa + outb) / 2.0\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 09:06:50.232641: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-29 09:06:50.385936: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-29 09:06:50.385973: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-29 09:06:50.388827: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-29 09:06:50.388864: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-29 09:06:50.388881: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-29 09:06:50.461653: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-29 09:06:50.461692: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-29 09:06:50.461697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-03-29 09:06:50.461719: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-03-29 09:06:50.462212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13512 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4080, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "n_classes = 3  # Ensure 'classes' is defined\n",
    "\n",
    "# Initialize the component models\n",
    "# modela_tf = TransferLearningModelNew(n_classes=n_classes)\n",
    "modelb_tf = DenseNet201(n_classes=n_classes)\n",
    "\n",
    "# Create the ensemble model\n",
    "# ensemble_model_tf = DenseNetEnsemble(modela=modela_tf, modelb=modelb_tf)\n",
    "model = modelb_tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, train_dataset, loss_fn):\n",
    "    train_loss = []\n",
    "    train_accuracy = []\n",
    "\n",
    "    for images, labels in train_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(images, training=True)\n",
    "            loss = loss_fn(labels, predictions)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        train_loss.append(loss)\n",
    "        correct = tf.reduce_sum(tf.cast(tf.equal(tf.argmax(predictions, axis=1), labels), tf.float32))\n",
    "        train_accuracy.append(correct / len(labels))\n",
    "    \n",
    "    return tf.reduce_mean(train_accuracy).numpy(), tf.reduce_mean(train_loss).numpy()\n",
    "\n",
    "def test_model(model, test_dataset, loss_fn):\n",
    "    test_loss = []\n",
    "    test_accuracy = []\n",
    "\n",
    "    for images, labels in test_dataset:\n",
    "        predictions = model(images, training=False)\n",
    "        loss = loss_fn(labels, predictions)\n",
    "        \n",
    "        test_loss.append(loss)\n",
    "        correct = tf.reduce_sum(tf.cast(tf.equal(tf.argmax(predictions, axis=1), labels), tf.float32))\n",
    "        test_accuracy.append(correct / len(labels))\n",
    "    \n",
    "    return tf.reduce_mean(test_accuracy).numpy(), tf.reduce_mean(test_loss).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=CONFIG.LEARNING_RATE)\n",
    "\n",
    "loss_function = tf.keras.losses.CategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Assuming CONFIG is a configuration object similar to the PyTorch setup\n",
    "# and you have x_train, y_train, x_val, y_val prepared\n",
    "\n",
    "model_name = \"ensemble\"\n",
    "best_model_path = f'{model_name}_epochs_{CONFIG.EPOCHS}_batchsize_{CONFIG.BATCH_SIZE}_lr_{CONFIG.LEARNING_RATE}.keras'\n",
    "\n",
    "# Prepare your datasets (assuming x_train, y_train, x_val, y_val are your datasets)\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(buffer_size=10000).batch(CONFIG.BATCH_SIZE)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(CONFIG.BATCH_SIZE)\n",
    "\n",
    "# Define the model, loss function, and optimizer\n",
    " # Initialize your TensorFlow model here\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=CONFIG.LEARNING_RATE),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(best_model_path, save_best_only=True, monitor='val_accuracy', mode='max', verbose=1),\n",
    "    EarlyStopping(monitor='val_accuracy', patience=10, verbose=1, mode='max', restore_best_weights=True)\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 150, 150, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1711698319.041356    3789 service.cc:145] XLA service 0x7f49040bb0d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1711698319.041415    3789 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 4080, Compute Capability 8.9\n",
      "2024-03-29 07:45:21.974634: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-03-29 07:45:30.938902: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1711698347.580613    4067 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_100123', 36 bytes spill stores, 36 bytes spill loads\n",
      "\n",
      "I0000 00:00:1711698513.131527    3789 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.3352 - loss: 1.1201\n",
      "Epoch 1: val_accuracy improved from -inf to 0.32617, saving model to ensemble_epochs_10_batchsize_8_lr_0.0001.keras\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m543s\u001b[0m 88ms/step - accuracy: 0.3352 - loss: 1.1201 - val_accuracy: 0.3262 - val_loss: 1.1125\n",
      "Epoch 2/10\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.3389 - loss: 1.1136\n",
      "Epoch 2: val_accuracy improved from 0.32617 to 0.33200, saving model to ensemble_epochs_10_batchsize_8_lr_0.0001.keras\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 81ms/step - accuracy: 0.3389 - loss: 1.1136 - val_accuracy: 0.3320 - val_loss: 1.1129\n",
      "Epoch 3/10\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.3337 - loss: 1.1131\n",
      "Epoch 3: val_accuracy did not improve from 0.33200\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 79ms/step - accuracy: 0.3337 - loss: 1.1131 - val_accuracy: 0.3245 - val_loss: 1.1068\n",
      "Epoch 4/10\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.3357 - loss: 1.1128\n",
      "Epoch 4: val_accuracy improved from 0.33200 to 0.33517, saving model to ensemble_epochs_10_batchsize_8_lr_0.0001.keras\n",
      "\u001b[1m3000/3000\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 82ms/step - accuracy: 0.3357 - loss: 1.1128 - val_accuracy: 0.3352 - val_loss: 1.1350\n",
      "Epoch 5/10\n",
      "\u001b[1m1059/3000\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:25\u001b[0m 75ms/step - accuracy: 0.3317 - loss: 1.1153"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(train_dataset, validation_data=val_dataset, epochs=CONFIG.EPOCHS, callbacks=callbacks)\n",
    "\n",
    "# Optionally, if you want to directly access the best accuracy achieved:\n",
    "best_accuracy = max(history.history['val_accuracy'])\n",
    "print(f\"Best validation accuracy: {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsoc12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
