{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 1, 150, 150) (30000,)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('train.pkl', 'rb') as f:\n",
    "    x, y = pickle.load(f)\n",
    "\n",
    "print(x.shape, y.shape)  # (number_of_samples, height, width) (number_of_samples,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y = to_categorical(y, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 3, 150, 150)\n",
      "(30000, 3, 150, 150)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Reshape X_train to the correct shape and duplicate the grayscale channel across 3 channels\n",
    "X_train_rgb = np.repeat(x, 3, axis=1)\n",
    "\n",
    "# Ensure the shape is now (24000, 3, 150, 150)\n",
    "print(X_train_rgb.shape)\n",
    "\n",
    "# If using TensorFlow, which expects the channel last format, you might need to transpose the axes\n",
    "x = np.transpose(X_train_rgb, (0, 2, 3, 1))\n",
    "\n",
    "# Ensure the shape is now (24000, 150, 150, 3) for TensorFlow\n",
    "print(X_train_rgb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 150, 150, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Initialize the ImageDataGenerator with only horizontal flip augmentation\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming `x` and `y` are your full dataset loaded from .npy files\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    horizontal_flip=True  # Enable horizontal flipping\n",
    "    # rescale=1./255  # Rescale pixel values\n",
    ")\n",
    "\n",
    "# Create a training data generator\n",
    "train_generator = train_datagen.flow(\n",
    "    x_train, y_train,\n",
    "    batch_size=32  # Adjust batch size as needed\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "     horizontal_flip=True\n",
    "    # rescale=1./255  # Rescale pixel values\n",
    ")\n",
    "\n",
    "# Create a test data generator\n",
    "test_generator = test_datagen.flow(\n",
    "    x_test, y_test,\n",
    "    batch_size=32,  # Adjust batch size as needed\n",
    "    shuffle=False  # Usually, you don't need to shuffle the test data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, Model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class TinyVGGTF(Model):\n",
    "    def __init__(self, input_shape, hidden_units, num_classes, use_skip_connections=True, use_batch_norm=True, use_dropout=True):\n",
    "        super(TinyVGGTF, self).__init__()\n",
    "        self.use_skip_connections = use_skip_connections\n",
    "\n",
    "        # Convolutional blocks\n",
    "        self.conv_block_1 = self._make_conv_block(hidden_units, use_batch_norm, use_dropout, first_block=True, input_shape=input_shape)\n",
    "        self.conv_block_2 = self._make_conv_block(hidden_units, use_batch_norm, use_dropout)\n",
    "\n",
    "        # Classifier\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense = layers.Dense(num_classes, activation='softmax')\n",
    "        \n",
    "    def _make_conv_block(self, hidden_units, use_batch_norm, use_dropout, first_block=False, input_shape=None):\n",
    "        layers_list = []\n",
    "        if first_block:\n",
    "            layers_list.append(layers.Conv2D(hidden_units, kernel_size=3, strides=1, padding='same', activation='relu', input_shape=input_shape))\n",
    "        else:\n",
    "            layers_list.append(layers.Conv2D(hidden_units, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
    "        if use_batch_norm:\n",
    "            layers_list.append(layers.BatchNormalization())\n",
    "        layers_list.append(layers.Conv2D(hidden_units, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
    "        if use_batch_norm:\n",
    "            layers_list.append(layers.BatchNormalization())\n",
    "        layers_list.append(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "        if use_dropout:\n",
    "            layers_list.append(layers.Dropout(0.25))\n",
    "\n",
    "        return models.Sequential(layers_list)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv_block_1(inputs)\n",
    "        block_1_output = x\n",
    "        x = self.conv_block_2(x)\n",
    "\n",
    "        if self.use_skip_connections:\n",
    "            # Resize block_1_output to match x's dimensions before adding\n",
    "            target_shape = x.shape[1:3]\n",
    "            resized_block_1_output = tf.image.resize(block_1_output, target_shape, method='nearest')\n",
    "            x = layers.Add()([x, resized_block_1_output])\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        return self.dense(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tiny_vggtf_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_6 (Sequential)   (None, 75, 75, 32)        10400     \n",
      "                                                                 \n",
      " sequential_7 (Sequential)   (1, 37, 37, 32)           18752     \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         multiple                  0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             multiple                  131427    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,579\n",
      "Trainable params: 160,323\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Adjust the parameters according to your dataset and requirements\n",
    "input_shape = (150, 150, 3)  # Input shape of your images\n",
    "hidden_units = 32  # Number of filters in the convolutional layers\n",
    "num_classes = 3  # Setting the number of classes to 3 as specified\n",
    "\n",
    "# Instantiate the model\n",
    "model = TinyVGGTF(input_shape, hidden_units, num_classes, use_skip_connections=True, use_batch_norm=True, use_dropout=True)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create a dummy input to build the model\n",
    "dummy_input = np.random.random((1, *input_shape)).astype(np.float32)\n",
    "\n",
    "# Pass the dummy input through the model to build it\n",
    "model(dummy_input)\n",
    "\n",
    "# Now, you can print the model summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: /physical_device:GPU:0   Type: GPU\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 3070 Laptop GPU, compute capability 8.6\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "\n",
    "set_global_policy('mixed_float16')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "750/750 [==============================] - 34s 45ms/step - loss: 2.2587 - accuracy: 0.3384 - val_loss: 1.6710 - val_accuracy: 0.3357\n",
      "Epoch 2/10\n",
      "750/750 [==============================] - 33s 44ms/step - loss: 1.9945 - accuracy: 0.3352 - val_loss: 2.0928 - val_accuracy: 0.3332\n",
      "Epoch 3/10\n",
      "750/750 [==============================] - 36s 49ms/step - loss: 1.5790 - accuracy: 0.3406 - val_loss: 1.4190 - val_accuracy: 0.3319\n",
      "Epoch 4/10\n",
      "750/750 [==============================] - 39s 52ms/step - loss: 1.2559 - accuracy: 0.3343 - val_loss: 1.3045 - val_accuracy: 0.3349\n",
      "Epoch 5/10\n",
      "750/750 [==============================] - 48s 64ms/step - loss: 1.1358 - accuracy: 0.3414 - val_loss: 1.1164 - val_accuracy: 0.3332\n",
      "Epoch 6/10\n",
      "750/750 [==============================] - 54s 72ms/step - loss: 1.1061 - accuracy: 0.3427 - val_loss: 1.1079 - val_accuracy: 0.3344\n",
      "Epoch 7/10\n",
      "750/750 [==============================] - 58s 77ms/step - loss: 1.1016 - accuracy: 0.3405 - val_loss: 1.1408 - val_accuracy: 0.3463\n",
      "Epoch 8/10\n",
      "750/750 [==============================] - 61s 82ms/step - loss: 1.1005 - accuracy: 0.3485 - val_loss: 1.0989 - val_accuracy: 0.3300\n",
      "Epoch 9/10\n",
      "750/750 [==============================] - 67s 89ms/step - loss: 1.1016 - accuracy: 0.3349 - val_loss: 1.1244 - val_accuracy: 0.3327\n",
      "Epoch 10/10\n",
      "750/750 [==============================] - 67s 89ms/step - loss: 1.1012 - accuracy: 0.3375 - val_loss: 1.0988 - val_accuracy: 0.3327\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(x_train) // 32,  # Ensure this matches your batch size\n",
    "    epochs=10,  # Adjust the number of epochs as needed\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=len(x_test) // 32  # Ensure this matches your batch size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
